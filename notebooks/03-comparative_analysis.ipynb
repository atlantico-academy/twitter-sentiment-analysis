{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb5df34-8a87-415f-85cf-97fdf4756c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/madson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importando bibliotecas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# ignorando warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# importando stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab5306d-a741-47d9-b3e4-f1b81596a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregamento conjunto de dados\n",
    "data_path = Path(\"../data/raw/data.csv\")\n",
    "\n",
    "# carregamento dicionário de dados\n",
    "dict_path = Path(\"../data/external/dicionario.csv\")\n",
    "\n",
    "datasets = [\n",
    "    (\"lemmatization_sem_stopwords\", Path('../data/processed/lemmatization_sem_stopwords.csv')),\n",
    "    (\"lemmatization_com_stopwords\", Path('../data/processed/lemmatization_com_stopwords.csv')),\n",
    "    (\"stemming_com_stopwords\", Path('../data/processed/stemming_com_stopwords.csv')),\n",
    "    (\"stemming_sem_stopwords\", Path('../data/processed/stemming_sem_stopwords.csv'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0affb4-03b9-4dc1-b682-5b5ce5672eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('../data/processed/lemmatization_sem_stopwords.csv')).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e10420-36d7-4df4-9b4c-a0d5d15f5536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Dados"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>1046795689781551111</td>\n",
       "      <td>@rafinhabastos Cabo Daciolo é O Cara!!! :D</td>\n",
       "      <td>2018-10-01 13:15:15-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84397</th>\n",
       "      <td>1039248939004964865</td>\n",
       "      <td>País ganha primeiro fundo de renda fixa com ap...</td>\n",
       "      <td>2018-09-10 17:27:10-03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>veja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77797</th>\n",
       "      <td>1043666255268450305</td>\n",
       "      <td>'O tempo não para': Samuca e Waleska quase se ...</td>\n",
       "      <td>2018-09-22 22:00:00-03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>jornaloglobo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>1050737799421448192</td>\n",
       "      <td>Feliz dia das crianças pra vc que tem essas at...</td>\n",
       "      <td>2018-10-12 10:19:47-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56753</th>\n",
       "      <td>1046783240294547456</td>\n",
       "      <td>is dis marupokkk? — Sobra :(( https://t.co/cb8...</td>\n",
       "      <td>2018-10-01 12:25:47-03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "19981  1046795689781551111         @rafinhabastos Cabo Daciolo é O Cara!!! :D   \n",
       "84397  1039248939004964865  País ganha primeiro fundo de renda fixa com ap...   \n",
       "77797  1043666255268450305  'O tempo não para': Samuca e Waleska quase se ...   \n",
       "1837   1050737799421448192  Feliz dia das crianças pra vc que tem essas at...   \n",
       "56753  1046783240294547456  is dis marupokkk? — Sobra :(( https://t.co/cb8...   \n",
       "\n",
       "                     tweet_date  sentiment    query_used  \n",
       "19981 2018-10-01 13:15:15-03:00          1            :)  \n",
       "84397 2018-09-10 17:27:10-03:00          2          veja  \n",
       "77797 2018-09-22 22:00:00-03:00          2  jornaloglobo  \n",
       "1837  2018-10-12 10:19:47-03:00          1            :)  \n",
       "56753 2018-10-01 12:25:47-03:00          0            :(  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Dicionário"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variavel</th>\n",
       "      <th>significado</th>\n",
       "      <th>tipo</th>\n",
       "      <th>valores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>ID único por usuário</td>\n",
       "      <td>useless</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweet_text</td>\n",
       "      <td>Texto publicado</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweet_date</td>\n",
       "      <td>Data de publicação</td>\n",
       "      <td>time</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentiment</td>\n",
       "      <td>Algorítmo de classificação do sentimento do us...</td>\n",
       "      <td>nominal</td>\n",
       "      <td>[0,1,2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>query_used</td>\n",
       "      <td>Palavra relevante</td>\n",
       "      <td>nominal</td>\n",
       "      <td>[':)', ':(', 'veja', 'jornaloglobo', 'g1', 'fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     variavel                                        significado     tipo  \\\n",
       "0          id                               ID único por usuário  useless   \n",
       "1  tweet_text                                    Texto publicado     text   \n",
       "2  tweet_date                                 Data de publicação     time   \n",
       "3   sentiment  Algorítmo de classificação do sentimento do us...  nominal   \n",
       "4  query_used                                  Palavra relevante  nominal   \n",
       "\n",
       "                                             valores  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                            [0,1,2]  \n",
       "4  [':)', ':(', 'veja', 'jornaloglobo', 'g1', 'fo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# leitura conjunto de dados\n",
    "df_data = pd.read_csv(data_path, sep=\";\").sample(100)\n",
    "\n",
    "# padronização tweet_date\n",
    "df_data['tweet_date'] = pd.to_datetime(df_data['tweet_date'])\n",
    "df_data['tweet_date'] = df_data.tweet_date.dt.tz_convert('Brazil/East')\n",
    "\n",
    "# visualização dados\n",
    "display(Markdown(\"### Dados\"))\n",
    "display(df_data.head())\n",
    "\n",
    "# leitura dicionário de dados\n",
    "df_dict = pd.read_csv(dict_path)\n",
    "\n",
    "# visualização dicionário de dados\n",
    "display(Markdown(\"### Dicionário\"))\n",
    "display(df_dict.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a58efc-41d6-40ca-810d-6fa47f97bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"sentiment\"\n",
    "useless_columns =  df_dict.query(\"tipo == 'useless'\").variavel.to_list()\n",
    "useless_columns.append('query_used')\n",
    "nominal_columns = (\n",
    "    df_dict\n",
    "    .query(\n",
    "        \"tipo == 'nominal' and \"\n",
    "        \"variavel not in @useless_columns and \"\n",
    "        \"variavel != @target_column\"\n",
    "    )\n",
    "    .variavel\n",
    "    .to_list()\n",
    ")\n",
    "text_columns = (\n",
    "    df_dict\n",
    "    .query(\n",
    "        \"tipo == 'text' and \"\n",
    "        \"variavel not in @useless_columns and \"\n",
    "        \"variavel != @target_column\"\n",
    "    )\n",
    "    .variavel\n",
    "    .to_list()\n",
    ")\n",
    "time_columns = (\n",
    "    df_dict\n",
    "    .query(\n",
    "        \"tipo == 'time' and \"\n",
    "        \"variavel not in @useless_columns and \"\n",
    "        \"variavel != @target_column\"\n",
    "    )\n",
    "    .variavel\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ca3ca-6293-408c-836b-e05500eff727",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_preprocessor = Pipeline([\n",
    "    # Tratamento de dados discrepantes\n",
    "    (\"missing\", SimpleImputer(strategy='most_frequent')), # Tratamento de dados faltantes\n",
    "    (\"encoder\", OneHotEncoder(sparse=False,handle_unknown='ignore')), # Codificação de variáveis\n",
    "    # Seleção de variáveis\n",
    "    (\"normalization\", StandardScaler()), # Normalização\n",
    "])\n",
    "text_preprocessor = Pipeline([\n",
    "    (\"bag of words\", Pipeline([('count', CountVectorizer(max_features=1000, strip_accents='ascii', lowercase=True)),('tfid', TfidfTransformer())])),\n",
    "    # Tratamento de dados faltantes\n",
    "    # Codificação de variáveis\n",
    "    # Seleção de variáveis\n",
    "    (\"pca\", TruncatedSVD(n_components=500, random_state=42)) # Redução de dimensionalidade - PCA\n",
    "    #(\"normalization\", StandardScaler()) # Normalização\n",
    "])\n",
    "time_preprocessor = Pipeline([\n",
    "    # Tratamento de dados discrepantes\n",
    "    # Tratamento de dados faltantes\n",
    "    (\"encoder\", OrdinalEncoder()), # Codificação de variáveis\n",
    "    # Seleção de variáveis\n",
    "    # Normalização\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716c84be-bf32-4fb8-8d9d-8977896d3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    # (\"nominal\", nominal_preprocessor, nominal_columns),\n",
    "    (\"text\", text_preprocessor, text_columns[0]), # setando a coluna tweet_text\n",
    "    #(\"time\", time_preprocessor, time_columns),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420ad3b4-9948-4e7f-b133-43ae37ff9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo parâmetros dos modelos\n",
    "models = [\n",
    "    (\n",
    "        \"LR\",\n",
    "        LogisticRegression(solver='liblinear', max_iter=1000),\n",
    "        {\"penalty\": ['l1', 'l2']}\n",
    "    ),\n",
    "    (\n",
    "        \"KNN\",\n",
    "        KNeighborsClassifier(metric='euclidean'),\n",
    "        {\"n_neighbors\": [3, 5, 11, 15]}\n",
    "    ),\n",
    "    (\n",
    "        \"SVM\",\n",
    "        SVC(max_iter=1000),\n",
    "        {\"kernel\": ['linear', 'rbf']}\n",
    "    ),\n",
    "    (\n",
    "        \"NB\",\n",
    "        GaussianNB(),\n",
    "        {'var_smoothing': np.logspace(0,-9, num=3)}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024c2f01-37eb-4a95-b6e0-57a699e3961f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definindo métricas\n",
    "metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='macro'),\n",
    "    'recall': make_scorer(recall_score, average='macro'),\n",
    "    'f1': make_scorer(f1_score, average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a921cd63-766a-487b-be23-961bb7f438ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo scoring para GridSearchCV\n",
    "scoring_metric = make_scorer(recall_score, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa0ee72-9840-436a-b13a-a780c5bae43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando características previsores da classe\n",
    "#X = df_data.drop(columns=[*useless_columns, target_column], axis=1)\n",
    "\n",
    "\n",
    "# Definindo cross validation\n",
    "cv = ShuffleSplit(n_splits=1, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dac0235-0c5d-4630-baf5-3dc4cc05bcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR-lemmatization_sem_stopwords run...\n",
      "KNN-lemmatization_sem_stopwords run...\n",
      "SVM-lemmatization_sem_stopwords run...\n",
      "NB-lemmatization_sem_stopwords run...\n",
      "LR-lemmatization_com_stopwords run...\n",
      "KNN-lemmatization_com_stopwords run...\n",
      "SVM-lemmatization_com_stopwords run...\n",
      "NB-lemmatization_com_stopwords run...\n",
      "LR-stemming_com_stopwords run...\n",
      "KNN-stemming_com_stopwords run...\n",
      "SVM-stemming_com_stopwords run...\n",
      "NB-stemming_com_stopwords run...\n",
      "LR-stemming_sem_stopwords run...\n",
      "KNN-stemming_sem_stopwords run...\n",
      "SVM-stemming_sem_stopwords run...\n",
      "NB-stemming_sem_stopwords run...\n"
     ]
    }
   ],
   "source": [
    "# Realizando treinamento dos modelos selecionados\n",
    "results = {}\n",
    "\n",
    "for dataset_name, dataset_path in datasets:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    X = df.drop(columns=[*useless_columns, *time_columns, target_column], axis=1)\n",
    "    y = df[[target_column]].to_numpy().ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.8, random_state=42, shuffle=True)\n",
    "\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    model_results = {}\n",
    "    for model_name, model_obj, model_params in models:\n",
    "        print(f'{model_name}-{dataset_name} run...')\n",
    "        model_gs = GridSearchCV(model_obj, model_params, scoring='accuracy')\n",
    "        # approach = Pipeline([\n",
    "        #     (\"preprocessing\", preprocessor),\n",
    "        #     (\"model\", model_gs)\n",
    "        # ])\n",
    "        # model_results = cross_validate(\n",
    "        #     approach,\n",
    "        #     X=X,\n",
    "        #     y=y,\n",
    "        #     scoring=metrics,\n",
    "        #     cv=cv,\n",
    "        #     n_jobs=4\n",
    "        # )\n",
    "        model_gs.fit(X_train_transformed, y_train)\n",
    "        y_hat = model_gs.predict(X_test_transformed)\n",
    "        \n",
    "        for metric_name, metric_func in metrics.items():\n",
    "            result = metric_func(model_gs, X_test_transformed, y_test)\n",
    "            if metric_name in model_results.keys():\n",
    "                model_results[metric_name] = np.append(model_results[metric_name], result)\n",
    "            else:\n",
    "                model_results[metric_name] = [result]\n",
    "        model_results['name'] = [f\"{model_name}-{dataset_name}\"] * len(model_results['precision'])\n",
    "        if results:\n",
    "            for key, value in model_results.items():\n",
    "                results[key] = np.append(results[key], value)\n",
    "        else:\n",
    "            results = model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f14b3c-6311-4817-a27b-3f5c42562235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataframe com os resultados\n",
    "df_results = pd.DataFrame(results)\n",
    "# df_results.groupby('name').agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd94a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3805f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc0334cb-9e3e-41af-a3b0-3e7ff74cd64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dd06f_row0_col5, #T_dd06f_row1_col5, #T_dd06f_row2_col5, #T_dd06f_row3_col5 {\n",
       "  color: white;\n",
       "  background-color: gray;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dd06f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_dd06f_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n",
       "      <th id=\"T_dd06f_level0_col1\" class=\"col_heading level0 col1\" >KNN-lemmatization_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col2\" class=\"col_heading level0 col2\" >KNN-stemming_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col3\" class=\"col_heading level0 col3\" >KNN-stemming_sem_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col4\" class=\"col_heading level0 col4\" >LR-lemmatization_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col5\" class=\"col_heading level0 col5\" >LR-stemming_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col6\" class=\"col_heading level0 col6\" >LR-stemming_sem_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col7\" class=\"col_heading level0 col7\" >NB-lemmatization_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col8\" class=\"col_heading level0 col8\" >NB-lemmatization_sem_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col9\" class=\"col_heading level0 col9\" >NB-stemming_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col10\" class=\"col_heading level0 col10\" >NB-stemming_sem_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col11\" class=\"col_heading level0 col11\" >SVM-lemmatization_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col12\" class=\"col_heading level0 col12\" >SVM-stemming_com_stopwords</th>\n",
       "      <th id=\"T_dd06f_level0_col13\" class=\"col_heading level0 col13\" >SVM-stemming_sem_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_dd06f_row0_col0\" class=\"data row0 col0\" >accuracy</td>\n",
       "      <td id=\"T_dd06f_row0_col1\" class=\"data row0 col1\" >0.739 ± 0.046</td>\n",
       "      <td id=\"T_dd06f_row0_col2\" class=\"data row0 col2\" >0.727 ± 0.061</td>\n",
       "      <td id=\"T_dd06f_row0_col3\" class=\"data row0 col3\" >0.722 ± 0.049</td>\n",
       "      <td id=\"T_dd06f_row0_col4\" class=\"data row0 col4\" >0.785 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row0_col5\" class=\"data row0 col5\" >0.789 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row0_col6\" class=\"data row0 col6\" >0.771 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row0_col7\" class=\"data row0 col7\" >0.691 ± 0.077</td>\n",
       "      <td id=\"T_dd06f_row0_col8\" class=\"data row0 col8\" >0.714 ± 0.050</td>\n",
       "      <td id=\"T_dd06f_row0_col9\" class=\"data row0 col9\" >0.695 ± 0.077</td>\n",
       "      <td id=\"T_dd06f_row0_col10\" class=\"data row0 col10\" >0.685 ± 0.071</td>\n",
       "      <td id=\"T_dd06f_row0_col11\" class=\"data row0 col11\" >0.731 ± 0.039</td>\n",
       "      <td id=\"T_dd06f_row0_col12\" class=\"data row0 col12\" >0.732 ± 0.050</td>\n",
       "      <td id=\"T_dd06f_row0_col13\" class=\"data row0 col13\" >0.721 ± 0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd06f_row1_col0\" class=\"data row1 col0\" >precision</td>\n",
       "      <td id=\"T_dd06f_row1_col1\" class=\"data row1 col1\" >0.741 ± 0.040</td>\n",
       "      <td id=\"T_dd06f_row1_col2\" class=\"data row1 col2\" >0.732 ± 0.053</td>\n",
       "      <td id=\"T_dd06f_row1_col3\" class=\"data row1 col3\" >0.732 ± 0.037</td>\n",
       "      <td id=\"T_dd06f_row1_col4\" class=\"data row1 col4\" >0.781 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row1_col5\" class=\"data row1 col5\" >0.785 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row1_col6\" class=\"data row1 col6\" >0.768 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row1_col7\" class=\"data row1 col7\" >0.690 ± 0.079</td>\n",
       "      <td id=\"T_dd06f_row1_col8\" class=\"data row1 col8\" >0.714 ± 0.049</td>\n",
       "      <td id=\"T_dd06f_row1_col9\" class=\"data row1 col9\" >0.697 ± 0.074</td>\n",
       "      <td id=\"T_dd06f_row1_col10\" class=\"data row1 col10\" >0.689 ± 0.071</td>\n",
       "      <td id=\"T_dd06f_row1_col11\" class=\"data row1 col11\" >0.732 ± 0.035</td>\n",
       "      <td id=\"T_dd06f_row1_col12\" class=\"data row1 col12\" >0.734 ± 0.043</td>\n",
       "      <td id=\"T_dd06f_row1_col13\" class=\"data row1 col13\" >0.727 ± 0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd06f_row2_col0\" class=\"data row2 col0\" >recall</td>\n",
       "      <td id=\"T_dd06f_row2_col1\" class=\"data row2 col1\" >0.739 ± 0.046</td>\n",
       "      <td id=\"T_dd06f_row2_col2\" class=\"data row2 col2\" >0.727 ± 0.061</td>\n",
       "      <td id=\"T_dd06f_row2_col3\" class=\"data row2 col3\" >0.722 ± 0.049</td>\n",
       "      <td id=\"T_dd06f_row2_col4\" class=\"data row2 col4\" >0.785 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row2_col5\" class=\"data row2 col5\" >0.789 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row2_col6\" class=\"data row2 col6\" >0.771 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row2_col7\" class=\"data row2 col7\" >0.691 ± 0.077</td>\n",
       "      <td id=\"T_dd06f_row2_col8\" class=\"data row2 col8\" >0.714 ± 0.050</td>\n",
       "      <td id=\"T_dd06f_row2_col9\" class=\"data row2 col9\" >0.695 ± 0.077</td>\n",
       "      <td id=\"T_dd06f_row2_col10\" class=\"data row2 col10\" >0.685 ± 0.071</td>\n",
       "      <td id=\"T_dd06f_row2_col11\" class=\"data row2 col11\" >0.731 ± 0.039</td>\n",
       "      <td id=\"T_dd06f_row2_col12\" class=\"data row2 col12\" >0.732 ± 0.051</td>\n",
       "      <td id=\"T_dd06f_row2_col13\" class=\"data row2 col13\" >0.721 ± 0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dd06f_row3_col0\" class=\"data row3 col0\" >f1</td>\n",
       "      <td id=\"T_dd06f_row3_col1\" class=\"data row3 col1\" >0.739 ± 0.043</td>\n",
       "      <td id=\"T_dd06f_row3_col2\" class=\"data row3 col2\" >0.728 ± 0.058</td>\n",
       "      <td id=\"T_dd06f_row3_col3\" class=\"data row3 col3\" >0.724 ± 0.045</td>\n",
       "      <td id=\"T_dd06f_row3_col4\" class=\"data row3 col4\" >0.783 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row3_col5\" class=\"data row3 col5\" >0.787 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row3_col6\" class=\"data row3 col6\" >0.769 ± 0.000</td>\n",
       "      <td id=\"T_dd06f_row3_col7\" class=\"data row3 col7\" >0.689 ± 0.080</td>\n",
       "      <td id=\"T_dd06f_row3_col8\" class=\"data row3 col8\" >0.713 ± 0.050</td>\n",
       "      <td id=\"T_dd06f_row3_col9\" class=\"data row3 col9\" >0.691 ± 0.082</td>\n",
       "      <td id=\"T_dd06f_row3_col10\" class=\"data row3 col10\" >0.681 ± 0.078</td>\n",
       "      <td id=\"T_dd06f_row3_col11\" class=\"data row3 col11\" >0.731 ± 0.037</td>\n",
       "      <td id=\"T_dd06f_row3_col12\" class=\"data row3 col12\" >0.732 ± 0.048</td>\n",
       "      <td id=\"T_dd06f_row3_col13\" class=\"data row3 col13\" >0.722 ± 0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5c95ab6c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "O melhor modelo é o : **LR-stemming_com_stopwords**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criando funções para selecionar o melhor modelo\n",
    "def highlight_max(s, props=''):\n",
    "    values = [float(value.split()[0]) for value in s.values[1:]]\n",
    "    result = [''] * len(s.values)\n",
    "    if s.values[0].endswith('time'):\n",
    "        result[np.argmin(values)+1] = props\n",
    "    else:\n",
    "        result[np.argmax(values)+1] = props\n",
    "    return result\n",
    "\n",
    "def get_winner(s):\n",
    "    metric = s.values[0]\n",
    "    values = [float(value.split()[0]) for value in s.values[1:]]\n",
    "    models = results.columns[1:]\n",
    "    \n",
    "    if s.values[0].endswith('time'):\n",
    "        return models[np.argmin(values)]\n",
    "    else:\n",
    "        return models[np.argmax(values)]\n",
    "\n",
    "results = (\n",
    "    pd\n",
    "    .DataFrame(df_results)\n",
    "    .groupby(['name'])\n",
    "    .agg([lambda x: f\"{np.mean(x):.3f} ± {np.std(x):.3f}\"])#\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"score\"})\n",
    "    .drop(columns=\"level_1\")\n",
    "    # .set_index('score')\n",
    ")\n",
    "time_scores = ['fit_time', 'score_time']\n",
    "winner = results.query('score not in @time_scores').apply(get_winner, axis=1).value_counts().index[0]\n",
    "results.columns.name = ''\n",
    "results = (\n",
    "    results\n",
    "    .style\n",
    "    .hide(axis='index')\n",
    "    .apply(highlight_max, props='color:white;background-color:gray', axis=1)\n",
    ")\n",
    "display(results)\n",
    "display(Markdown(f'O melhor modelo é o : **{winner}**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f83bd67-cb59-4896-9d50-ebe05c540d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;bag of &#x27;\n",
       "                                                                   &#x27;words&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;count&#x27;,\n",
       "                                                                                    CountVectorizer(max_features=1000,\n",
       "                                                                                                    strip_accents=&#x27;ascii&#x27;)),\n",
       "                                                                                   (&#x27;tfid&#x27;,\n",
       "                                                                                    TfidfTransformer())])),\n",
       "                                                                  (&#x27;pca&#x27;,\n",
       "                                                                   TruncatedSVD(n_components=500,\n",
       "                                                                                random_state=42))]),\n",
       "                                                  &#x27;tweet_text&#x27;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 GridSearchCV(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                           solver=&#x27;liblinear&#x27;),\n",
       "                              param_grid={&#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "                              scoring=&#x27;accuracy&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;bag of &#x27;\n",
       "                                                                   &#x27;words&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;count&#x27;,\n",
       "                                                                                    CountVectorizer(max_features=1000,\n",
       "                                                                                                    strip_accents=&#x27;ascii&#x27;)),\n",
       "                                                                                   (&#x27;tfid&#x27;,\n",
       "                                                                                    TfidfTransformer())])),\n",
       "                                                                  (&#x27;pca&#x27;,\n",
       "                                                                   TruncatedSVD(n_components=500,\n",
       "                                                                                random_state=42))]),\n",
       "                                                  &#x27;tweet_text&#x27;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 GridSearchCV(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                           solver=&#x27;liblinear&#x27;),\n",
       "                              param_grid={&#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "                              scoring=&#x27;accuracy&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;bag of words&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;count&#x27;,\n",
       "                                                                   CountVectorizer(max_features=1000,\n",
       "                                                                                   strip_accents=&#x27;ascii&#x27;)),\n",
       "                                                                  (&#x27;tfid&#x27;,\n",
       "                                                                   TfidfTransformer())])),\n",
       "                                                 (&#x27;pca&#x27;,\n",
       "                                                  TruncatedSVD(n_components=500,\n",
       "                                                               random_state=42))]),\n",
       "                                 &#x27;tweet_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>tweet_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">bag of words: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;count&#x27;,\n",
       "                 CountVectorizer(max_features=1000, strip_accents=&#x27;ascii&#x27;)),\n",
       "                (&#x27;tfid&#x27;, TfidfTransformer())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=1000, strip_accents=&#x27;ascii&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=500, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">model: GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;),\n",
       "             param_grid={&#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]}, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('text',\n",
       "                                                  Pipeline(steps=[('bag of '\n",
       "                                                                   'words',\n",
       "                                                                   Pipeline(steps=[('count',\n",
       "                                                                                    CountVectorizer(max_features=1000,\n",
       "                                                                                                    strip_accents='ascii')),\n",
       "                                                                                   ('tfid',\n",
       "                                                                                    TfidfTransformer())])),\n",
       "                                                                  ('pca',\n",
       "                                                                   TruncatedSVD(n_components=500,\n",
       "                                                                                random_state=42))]),\n",
       "                                                  'tweet_text')])),\n",
       "                ('model',\n",
       "                 GridSearchCV(estimator=LogisticRegression(max_iter=1000,\n",
       "                                                           solver='liblinear'),\n",
       "                              param_grid={'penalty': ['l1', 'l2']},\n",
       "                              scoring='accuracy'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando treino do modelo completo\n",
    "model_winner = winner.split(\"-\")[0]\n",
    "dataset_winner = winner.split(\"-\")[1]\n",
    "model_name, model, model_params  = [foo for foo in models if foo[0] == model_winner][0]\n",
    "\n",
    "dataset_name, dataset_path  = [foo for foo in datasets if foo[0] == dataset_winner][0]\n",
    "\n",
    "df = pd.read_csv(dataset_path).sample(frac=1)\n",
    "X = df.drop(columns=[*useless_columns, *time_columns, target_column], axis=1)\n",
    "y = df[[target_column]].to_numpy().ravel()\n",
    "\n",
    "\n",
    "model_gs = GridSearchCV(model, model_params, scoring='accuracy')\n",
    "approach = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", model_gs)\n",
    "])\n",
    "\n",
    "\n",
    "approach.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c876a9ac-ce0f-4975-9787-d98f2f196b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvando modelo treinado\n",
    "joblib.dump(approach, '../models/model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932f8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bff4f1a6af534c48cff79341307bffe7bcb705e4bddc169ca0749100feadcb0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
