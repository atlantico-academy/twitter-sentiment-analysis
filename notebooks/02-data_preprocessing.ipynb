{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento conjunto de dados\n",
    "data_path = Path(\"../data/raw/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27891</th>\n",
       "      <td>1045428393611808768</td>\n",
       "      <td>Ou seja se encomendar as yeezy vou pagar 50 eu...</td>\n",
       "      <td>Thu Sep 27 21:42:06 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>1049284285230931969</td>\n",
       "      <td>@duducaralho @brendaccarvalho ótimo. agora vam...</td>\n",
       "      <td>Mon Oct 08 13:04:03 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37430</th>\n",
       "      <td>1049282536524922881</td>\n",
       "      <td>@Dvrkskinsayan :( eu não percebo desculpa</td>\n",
       "      <td>Mon Oct 08 12:57:06 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93208</th>\n",
       "      <td>1038587601542963203</td>\n",
       "      <td>Analisando #FAKE ou #FATO sobre a chamada do @...</td>\n",
       "      <td>Sun Sep 09 00:39:14 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>#fato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12352</th>\n",
       "      <td>1047557531315388416</td>\n",
       "      <td>@SantoosIbra @BrunoMsbe @futtmais @PortalMessi...</td>\n",
       "      <td>Wed Oct 03 18:42:32 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>1048423407019352065</td>\n",
       "      <td>opa solteira :) pena que es muito novinha kkk ...</td>\n",
       "      <td>Sat Oct 06 04:03:13 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69076</th>\n",
       "      <td>1049305919182393345</td>\n",
       "      <td>&amp;gt;@EstadaoPolitica Bolsonaro defende Paulo G...</td>\n",
       "      <td>Mon Oct 08 14:30:00 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>estadao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14749</th>\n",
       "      <td>1047487432617316353</td>\n",
       "      <td>@019matilde Tu sabi Mozão :))</td>\n",
       "      <td>Wed Oct 03 14:03:59 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39880</th>\n",
       "      <td>1049236837578018816</td>\n",
       "      <td>Eu só queria f1 antes de entrar na aula :(</td>\n",
       "      <td>Mon Oct 08 09:55:30 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88643</th>\n",
       "      <td>1052566326705713152</td>\n",
       "      <td>\"InBrands em busca de um sócio\" https://t.co/A...</td>\n",
       "      <td>Wed Oct 17 14:25:42 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>#trabalho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "27891  1045428393611808768  Ou seja se encomendar as yeezy vou pagar 50 eu...   \n",
       "4376   1049284285230931969  @duducaralho @brendaccarvalho ótimo. agora vam...   \n",
       "37430  1049282536524922881          @Dvrkskinsayan :( eu não percebo desculpa   \n",
       "93208  1038587601542963203  Analisando #FAKE ou #FATO sobre a chamada do @...   \n",
       "12352  1047557531315388416  @SantoosIbra @BrunoMsbe @futtmais @PortalMessi...   \n",
       "10153  1048423407019352065  opa solteira :) pena que es muito novinha kkk ...   \n",
       "69076  1049305919182393345  &gt;@EstadaoPolitica Bolsonaro defende Paulo G...   \n",
       "14749  1047487432617316353                      @019matilde Tu sabi Mozão :))   \n",
       "39880  1049236837578018816         Eu só queria f1 antes de entrar na aula :(   \n",
       "88643  1052566326705713152  \"InBrands em busca de um sócio\" https://t.co/A...   \n",
       "\n",
       "                           tweet_date  sentiment query_used  \n",
       "27891  Thu Sep 27 21:42:06 +0000 2018          1         :)  \n",
       "4376   Mon Oct 08 13:04:03 +0000 2018          1         :)  \n",
       "37430  Mon Oct 08 12:57:06 +0000 2018          0         :(  \n",
       "93208  Sun Sep 09 00:39:14 +0000 2018          2      #fato  \n",
       "12352  Wed Oct 03 18:42:32 +0000 2018          1         :)  \n",
       "10153  Sat Oct 06 04:03:13 +0000 2018          1         :)  \n",
       "69076  Mon Oct 08 14:30:00 +0000 2018          2    estadao  \n",
       "14749  Wed Oct 03 14:03:59 +0000 2018          1         :)  \n",
       "39880  Mon Oct 08 09:55:30 +0000 2018          0         :(  \n",
       "88643  Wed Oct 17 14:25:42 +0000 2018          2  #trabalho  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura do conjunto de dados\n",
    "df = pd.read_csv(data_path, sep=\";\").sample(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27891</th>\n",
       "      <td>1045428393611808768</td>\n",
       "      <td>Ou seja se encomendar as yeezy vou pagar 50 eu...</td>\n",
       "      <td>2018-09-27 18:42:06-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4376</th>\n",
       "      <td>1049284285230931969</td>\n",
       "      <td>@duducaralho @brendaccarvalho ótimo. agora vam...</td>\n",
       "      <td>2018-10-08 10:04:03-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37430</th>\n",
       "      <td>1049282536524922881</td>\n",
       "      <td>@Dvrkskinsayan :( eu não percebo desculpa</td>\n",
       "      <td>2018-10-08 09:57:06-03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93208</th>\n",
       "      <td>1038587601542963203</td>\n",
       "      <td>Analisando #FAKE ou #FATO sobre a chamada do @...</td>\n",
       "      <td>2018-09-08 21:39:14-03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>#fato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12352</th>\n",
       "      <td>1047557531315388416</td>\n",
       "      <td>@SantoosIbra @BrunoMsbe @futtmais @PortalMessi...</td>\n",
       "      <td>2018-10-03 15:42:32-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>1048423407019352065</td>\n",
       "      <td>opa solteira :) pena que es muito novinha kkk ...</td>\n",
       "      <td>2018-10-06 01:03:13-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69076</th>\n",
       "      <td>1049305919182393345</td>\n",
       "      <td>&amp;gt;@EstadaoPolitica Bolsonaro defende Paulo G...</td>\n",
       "      <td>2018-10-08 11:30:00-03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>estadao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14749</th>\n",
       "      <td>1047487432617316353</td>\n",
       "      <td>@019matilde Tu sabi Mozão :))</td>\n",
       "      <td>2018-10-03 11:03:59-03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39880</th>\n",
       "      <td>1049236837578018816</td>\n",
       "      <td>Eu só queria f1 antes de entrar na aula :(</td>\n",
       "      <td>2018-10-08 06:55:30-03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88643</th>\n",
       "      <td>1052566326705713152</td>\n",
       "      <td>\"InBrands em busca de um sócio\" https://t.co/A...</td>\n",
       "      <td>2018-10-17 11:25:42-03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>#trabalho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                         tweet_text  \\\n",
       "27891  1045428393611808768  Ou seja se encomendar as yeezy vou pagar 50 eu...   \n",
       "4376   1049284285230931969  @duducaralho @brendaccarvalho ótimo. agora vam...   \n",
       "37430  1049282536524922881          @Dvrkskinsayan :( eu não percebo desculpa   \n",
       "93208  1038587601542963203  Analisando #FAKE ou #FATO sobre a chamada do @...   \n",
       "12352  1047557531315388416  @SantoosIbra @BrunoMsbe @futtmais @PortalMessi...   \n",
       "10153  1048423407019352065  opa solteira :) pena que es muito novinha kkk ...   \n",
       "69076  1049305919182393345  &gt;@EstadaoPolitica Bolsonaro defende Paulo G...   \n",
       "14749  1047487432617316353                      @019matilde Tu sabi Mozão :))   \n",
       "39880  1049236837578018816         Eu só queria f1 antes de entrar na aula :(   \n",
       "88643  1052566326705713152  \"InBrands em busca de um sócio\" https://t.co/A...   \n",
       "\n",
       "                     tweet_date  sentiment query_used  \n",
       "27891 2018-09-27 18:42:06-03:00          1         :)  \n",
       "4376  2018-10-08 10:04:03-03:00          1         :)  \n",
       "37430 2018-10-08 09:57:06-03:00          0         :(  \n",
       "93208 2018-09-08 21:39:14-03:00          2      #fato  \n",
       "12352 2018-10-03 15:42:32-03:00          1         :)  \n",
       "10153 2018-10-06 01:03:13-03:00          1         :)  \n",
       "69076 2018-10-08 11:30:00-03:00          2    estadao  \n",
       "14749 2018-10-03 11:03:59-03:00          1         :)  \n",
       "39880 2018-10-08 06:55:30-03:00          0         :(  \n",
       "88643 2018-10-17 11:25:42-03:00          2  #trabalho  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correção do fuso horário\n",
    "df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "df['tweet_date'] = df.tweet_date.dt.tz_convert('Brazil/East')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando o conversor de texto para token\n",
    "countvec = CountVectorizer(strip_accents='ascii', \n",
    "                      lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando as frases em tokens\n",
    "bag_of_words = countvec.fit_transform(df.tweet_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>019matilde</th>\n",
       "      <th>50</th>\n",
       "      <th>afvbttexu7</th>\n",
       "      <th>agora</th>\n",
       "      <th>agressor</th>\n",
       "      <th>algo</th>\n",
       "      <th>analisando</th>\n",
       "      <th>antes</th>\n",
       "      <th>ao</th>\n",
       "      <th>arma</th>\n",
       "      <th>...</th>\n",
       "      <th>um</th>\n",
       "      <th>uma</th>\n",
       "      <th>vai</th>\n",
       "      <th>vamos</th>\n",
       "      <th>ver</th>\n",
       "      <th>visao</th>\n",
       "      <th>vivo</th>\n",
       "      <th>volta</th>\n",
       "      <th>vou</th>\n",
       "      <th>yeezy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   019matilde  50  afvbttexu7  agora  agressor  algo  analisando  antes  ao  \\\n",
       "0           0   1           0      0         0     0           0      0   0   \n",
       "1           0   0           0      2         0     1           0      0   0   \n",
       "2           0   0           0      0         0     0           0      0   0   \n",
       "3           0   0           0      0         1     0           1      0   0   \n",
       "4           0   0           0      0         0     0           0      0   1   \n",
       "5           0   0           0      0         0     0           0      0   0   \n",
       "6           0   0           0      0         0     0           0      0   0   \n",
       "7           1   0           0      0         0     0           0      0   0   \n",
       "8           0   0           0      0         0     0           0      1   0   \n",
       "9           0   0           1      0         0     0           0      0   0   \n",
       "\n",
       "   arma  ...  um  uma  vai  vamos  ver  visao  vivo  volta  vou  yeezy  \n",
       "0     0  ...   0    0    0      0    0      0     0      0    1      1  \n",
       "1     0  ...   1    1    1      1    0      1     0      1    0      0  \n",
       "2     0  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "3     1  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "4     0  ...   0    0    2      0    1      0     1      0    0      0  \n",
       "5     0  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "6     0  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "7     0  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "8     0  ...   0    0    0      0    0      0     0      0    0      0  \n",
       "9     0  ...   1    0    0      0    0      0     0      0    0      0  \n",
       "\n",
       "[10 rows x 134 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando dataframe com as contagens das palavras\n",
    "bow = pd.DataFrame(bag_of_words.toarray(), columns=countvec.get_feature_names_out())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de          7\n",
       "https       5\n",
       "co          5\n",
       "que         5\n",
       "vai         3\n",
       "           ..\n",
       "es          1\n",
       "era         1\n",
       "entrar      1\n",
       "entender    1\n",
       "yeezy       1\n",
       "Length: 134, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palavras mais frequentes\n",
    "frequente = bow.sum()\n",
    "frequente.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importações\n",
    "import nltk\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando objeto stemmer\n",
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    [Ou, seja, se, encomendar, as, yeezy, vou, pag...\n",
       "4376     [@, duducaralho, @, brendaccarvalho, ótimo, .,...\n",
       "37430    [@, Dvrkskinsayan, :, (, eu, não, percebo, des...\n",
       "93208    [Analisando, #, FAKE, ou, #, FATO, sobre, a, c...\n",
       "12352    [@, SantoosIbra, @, BrunoMsbe, @, futtmais, @,...\n",
       "10153    [opa, solteira, :, ), pena, que, es, muito, no...\n",
       "69076    [&, gt, ;, @, EstadaoPolitica, Bolsonaro, defe...\n",
       "14749            [@, 019matilde, Tu, sabi, Mozão, :, ), )]\n",
       "39880    [Eu, só, queria, f1, antes, de, entrar, na, au...\n",
       "88643    [``, InBrands, em, busca, de, um, sócio, '', h...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizando frases \n",
    "word_tokens = df.apply(lambda row: word_tokenize(row[\"tweet_text\"]), axis=1)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    [ou, sej, se, encomend, as, yeezy, vou, pag, 5...\n",
       "4376     [@, duducaralh, @, brendaccarvalh, ótim, ., ag...\n",
       "37430    [@, dvrkskinsayan, :, (, eu, não, perceb, desc...\n",
       "93208    [analis, #, fak, ou, #, fat, sobr, a, cham, do...\n",
       "12352    [@, santoosibr, @, brunomsb, @, futtm, @, port...\n",
       "10153    [opa, solt, :, ), pen, que, es, muit, nov, kkk...\n",
       "69076    [&, gt, ;, @, estadaopoli, bolsonar, defend, p...\n",
       "14749                 [@, 019matild, tu, sab, mo, :, ), )]\n",
       "39880     [eu, só, quer, f1, ant, de, entr, na, aul, :, (]\n",
       "88643    [``, inbrand, em, busc, de, um, sóci, '', http...\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando stemmer nas frases\n",
    "frase_stemmer = word_tokens.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "frase_stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalando spacy pt\n",
    "#!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importações\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    ([, ', Ou, ', ,, ', seja, ', ,, ', se, ', ,, '...\n",
       "4376     ([, ', @, ', ,, ', duducaralho, ', ,, ', @, ',...\n",
       "37430    ([, ', @, ', ,, ', Dvrkskinsayan, ', ,, ', :, ...\n",
       "93208    ([, ', Analisando, ', ,, ', #, ', ,, ', FAKE, ...\n",
       "12352    ([, ', @, ', ,, ', SantoosIbra, ', ,, ', @, ',...\n",
       "10153    ([, ', opa, ', ,, ', solteira, ', ,, ', :, ', ...\n",
       "69076    ([, ', &, ', ,, ', gt, ', ,, ', ;, ', ,, ', @,...\n",
       "14749    ([, ', @, ', ,, ', 019matilde, ', ,, ', Tu, ',...\n",
       "39880    ([, ', Eu, ', ,, ', só, ', ,, ', queria, ', ,,...\n",
       "88643    ([, ', `, `, ', ,, ', InBrands, ', ,, ', em, '...\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criando lista com todas as palavras da primeira frase\n",
    "doc = word_tokens.apply(lambda x: nlp(str([y for y in x])))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    ['ou','ser','se','encomendar','o','yeezy','ir'...\n",
       "4376     ['@','duducaralho','@','brendaccarvalho','bom'...\n",
       "37430    ['@','Dvrkskinsayan',':','(','eu','não','perce...\n",
       "93208    ['analisar','#','FAKE','ou','#','FATO','sobre'...\n",
       "12352    ['@','SantoosIbra','@','BrunoMsbe','@','futtma...\n",
       "10153    ['opa','solteira',':',')','pena','que','es','m...\n",
       "69076    ['&','gt',';','@','EstadaoPolitica','Bolsonaro...\n",
       "14749    ['@','019matilde','Tu','sabi','Mozão',':',')',...\n",
       "39880    ['eu','só','querer','f1','antes','de','entrar'...\n",
       "88643    ['``','InBrands','em','busca','de','um','sócio...\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando lematização\n",
    "frase_lema = doc.apply(lambda row: \"\".join([w.lemma_ for w in row]))\n",
    "frase_lema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    [encomendar, yeezy, vou, pagar, 50, euros, por...\n",
       "4376     [duducaralho, brendaccarvalho, ótimo, agora, v...\n",
       "37430                   [Dvrkskinsayan, percebo, desculpa]\n",
       "93208    [Analisando, FAKE, FATO, sobre, chamada, Jorna...\n",
       "12352    [SantoosIbra, BrunoMsbe, futtmais, PortalMessi...\n",
       "10153    [opa, solteira, pena, es, novinha, kkk, —, Ih,...\n",
       "69076    [gt, EstadaoPolitica, Bolsonaro, defende, Paul...\n",
       "14749                            [019matilde, sabi, Mozão]\n",
       "39880                    [queria, f1, antes, entrar, aula]\n",
       "88643    [``, InBrands, busca, sócio, '', https, //t.co...\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_sem_stopword = word_tokens.apply(lambda x: [word for word in x if not word.lower() in stop_words])\n",
    "frase_sem_stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891           [encomend, yeezy, vou, pag, 50, eur, port]\n",
       "4376     [duducaralh, brendaccarvalh, ótim, agor, vam, ...\n",
       "37430                     [dvrkskinsayan, perceb, desculp]\n",
       "93208    [analis, fak, fat, sobr, cham, jornaloglob, so...\n",
       "12352    [santoosibr, brunomsb, futtm, portalmess, vai,...\n",
       "10153    [opa, solt, pen, es, nov, kkk, —, ih, http, //...\n",
       "69076    [gt, estadaopoli, bolsonar, defend, paul, gued...\n",
       "14749                                 [019matild, sab, mo]\n",
       "39880                           [quer, f1, ant, entr, aul]\n",
       "88643    [``, inbrand, busc, sóci, '', http, //t.co/afv...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando stemmer nas frases\n",
    "frase_stemmer_sem_stopword = frase_sem_stopword.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "frase_stemmer_sem_stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    ([, ', encomendar, ', ,, ', yeezy, ', ,, ', vo...\n",
       "4376     ([, ', duducaralho, ', ,, ', brendaccarvalho, ...\n",
       "37430    ([, ', Dvrkskinsayan, ', ,, ', percebo, ', ,, ...\n",
       "93208    ([, ', Analisando, ', ,, ', FAKE, ', ,, ', FAT...\n",
       "12352    ([, ', SantoosIbra, ', ,, ', BrunoMsbe, ', ,, ...\n",
       "10153    ([, ', opa, ', ,, ', solteira, ', ,, ', pena, ...\n",
       "69076    ([, ', gt, ', ,, ', EstadaoPolitica, ', ,, ', ...\n",
       "14749    ([, ', 019matilde, ', ,, ', sabi, ', ,, ', Moz...\n",
       "39880    ([, ', queria, ', ,, ', f1, ', ,, ', antes, ',...\n",
       "88643    ([, ', `, `, ', ,, ', InBrands, ', ,, ', busca...\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criando lista com todas as palavras da primeira frase\n",
    "doc_sem_stopword = frase_sem_stopword.apply(lambda x: nlp(str([y for y in x])))\n",
    "doc_sem_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27891    ['encomendar','yeezy','ir','pagar','50','euro'...\n",
       "4376     ['duducaralho','brendaccarvalho','bom','agora'...\n",
       "37430               ['Dvrkskinsayan','percebo','desculpa'_\n",
       "93208    ['analisar','FAKE','FATO','sobre','chamar','Jo...\n",
       "12352    ['SantoosIbra','BrunoMsbe','futtmal','PortalMe...\n",
       "10153    ['opa','solteira','pena','es','novinho','kkk',...\n",
       "69076    ['gt','EstadaoPolitica','Bolsonaro','defender'...\n",
       "14749                        ['019matilde','sabi','Mozão'_\n",
       "39880              ['querer','f1','antes','entrar','aula'_\n",
       "88643    ['``','InBrands','busca','sócio',\"''\",'https',...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando lematização\n",
    "frase_lema = doc_sem_stopword.apply(lambda row: \"\".join([w.lemma_ for w in row]))\n",
    "frase_lema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
