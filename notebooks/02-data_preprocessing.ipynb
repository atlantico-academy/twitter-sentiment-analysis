{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fcd1ad-bbb0-4fd9-809c-117015fbfaf1",
   "metadata": {},
   "source": [
    "# Pré-processamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498393-545e-422f-a749-5b8424ac2314",
   "metadata": {},
   "source": [
    "## Carregar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5f136b-3b22-491b-b2d7-3ec052f48fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/amanda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import Markdown\n",
    "from string import punctuation\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "!python -m spacy download pt\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68656b-a9c8-4ab0-b617-4e9211a1d2ce",
   "metadata": {},
   "source": [
    "## Leitura do arquivo de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dba3dc4-92d0-4b7f-9a37-8ec001a4b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1050785521201541121</td>\n",
       "      <td>@Laranjito76 A pessoa certa para isso seria o ...</td>\n",
       "      <td>Fri Oct 12 16:29:25 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1050785431955140608</td>\n",
       "      <td>@behin_d_curtain Para mim, é precisamente o co...</td>\n",
       "      <td>Fri Oct 12 16:29:04 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1050785401248645120</td>\n",
       "      <td>Vou fazer um video hoje... estou pensando em f...</td>\n",
       "      <td>Fri Oct 12 16:28:56 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1050785370982547461</td>\n",
       "      <td>aaaaaaaa amei tanto essas polaroids, nem sei e...</td>\n",
       "      <td>Fri Oct 12 16:28:49 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1050785368902131713</td>\n",
       "      <td>Valoriza o coração do menininho que vc tem. El...</td>\n",
       "      <td>Fri Oct 12 16:28:49 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         tweet_text  \\\n",
       "0  1050785521201541121  @Laranjito76 A pessoa certa para isso seria o ...   \n",
       "1  1050785431955140608  @behin_d_curtain Para mim, é precisamente o co...   \n",
       "2  1050785401248645120  Vou fazer um video hoje... estou pensando em f...   \n",
       "3  1050785370982547461  aaaaaaaa amei tanto essas polaroids, nem sei e...   \n",
       "4  1050785368902131713  Valoriza o coração do menininho que vc tem. El...   \n",
       "\n",
       "                       tweet_date  sentiment query_used  \n",
       "0  Fri Oct 12 16:29:25 +0000 2018          1         :)  \n",
       "1  Fri Oct 12 16:29:04 +0000 2018          1         :)  \n",
       "2  Fri Oct 12 16:28:56 +0000 2018          1         :)  \n",
       "3  Fri Oct 12 16:28:49 +0000 2018          1         :)  \n",
       "4  Fri Oct 12 16:28:49 +0000 2018          1         :)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../data/raw/data.csv\")\n",
    "df = pd.read_csv(data_path, sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf0658-612f-4d23-bdcd-1a0fe0c42992",
   "metadata": {},
   "source": [
    "## Carregar dados utilizando o parâmetro chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fb2c03-9227-426b-8e36-ba75b1e55066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=\";\", chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746f127-01c7-4a19-af8f-bab69e970ebf",
   "metadata": {},
   "source": [
    "## Stemming e lemmatization com stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5e8b53-98cd-49e3-99de-e7584e7c997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conjunto de dados tweet_text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    @Laranjito76 A pessoa certa para isso seria o ...\n",
       "1    @behin_d_curtain Para mim, é precisamente o co...\n",
       "2    Vou fazer um video hoje... estou pensando em f...\n",
       "3    aaaaaaaa amei tanto essas polaroids, nem sei e...\n",
       "4    Valoriza o coração do menininho que vc tem. El...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Frases tokenizadas"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    [@, Laranjito76, A, pessoa, certa, para, isso,...\n",
       "1    [@, behin_d_curtain, Para, mim, ,, é, precisam...\n",
       "2    [Vou, fazer, um, video, hoje, ..., estou, pens...\n",
       "3    [aaaaaaaa, amei, tanto, essas, polaroids, ,, n...\n",
       "4    [Valoriza, o, coração, do, menininho, que, vc,...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stemming com stopwords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    [@, laranjito76, a, pesso, cert, par, iss, ser...\n",
       "1    [@, behin_d_curtain, par, mim, ,, é, precis, o...\n",
       "2    [vou, faz, um, vide, hoj, ..., est, pens, em, ...\n",
       "3    [aaaaaaa, ame, tant, ess, polaroid, ,, nem, se...\n",
       "4    [valoriz, o, coraçã, do, menin, que, vc, tem, ...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Lemmatization com stopwords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    ['@','Laranjito76','o','pessoa','certo','para'...\n",
       "1    ['@','behin_d_curtain','para','eu',',','ser','...\n",
       "2    ['Vou','fazer','um','video','hoje','...','esta...\n",
       "3    ['aaaaaaaar','ameir','tanto','esse','polaroids...\n",
       "4    ['Valoriza','o','coração','de o','menininho','...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stemmer = []\n",
    "df_lemma = []\n",
    "\n",
    "for i,data in enumerate(df):\n",
    "    # Correção do fuso horário\n",
    "    data['tweet_date'] = pd.to_datetime(data['tweet_date'])\n",
    "    data['tweet_date'] = data.tweet_date.dt.tz_convert('Brazil/East')\n",
    "    \n",
    "    if i == 0:\n",
    "        display(Markdown(\"### Conjunto de dados tweet_text\"))\n",
    "        display(data['tweet_text'].head())\n",
    "    \n",
    "    # Bag of words\n",
    "    countvec = CountVectorizer()\n",
    "    \n",
    "    # Transformando as frases em tokens\n",
    "    bag_of_words = countvec.fit_transform(data.tweet_text.tolist())\n",
    "    \n",
    "    # Criando dataframe com as contagens das palavras\n",
    "    bow = pd.DataFrame(bag_of_words.toarray(), columns=countvec.get_feature_names_out())\n",
    "    \n",
    "    # Palavras mais frequentes\n",
    "    frequente = bow.sum()\n",
    "    frequente.sort_values(ascending=False)\n",
    "    \n",
    "    # Tokenizando frases \n",
    "    word_tokens = data.apply(lambda row: word_tokenize(row[\"tweet_text\"]), axis=1)\n",
    "    \n",
    "    if i == 0:\n",
    "        display(Markdown(\"### Frases tokenizadas\"))\n",
    "        display(word_tokens.head())\n",
    "        \n",
    "        \n",
    "    #Stemming \n",
    "    # Criando objeto stemmer\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "        \n",
    "    # Realizando stemmer nas frases\n",
    "    frase_stemmer = word_tokens.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    \n",
    "    # Adicionando frases stemming ao dataframe\n",
    "    stemmer = data.drop(columns=['tweet_text'], axis=1)\n",
    "    stemmer['tweet_text'] = frase_stemmer\n",
    "    df_stemmer.append(stemmer)\n",
    "    \n",
    "    if i == 0:\n",
    "        display(Markdown(\"### Stemming com stopwords\"))\n",
    "        display(frase_stemmer.head())\n",
    "   \n",
    "    # Criando lista com todas as palavras\n",
    "    doc = word_tokens.apply(lambda x: nlp(str([y for y in x])))\n",
    "    \n",
    "    # Realizando lematização\n",
    "    frase_lemma = doc.apply(lambda row: \"\".join([w.lemma_ for w in row]))\n",
    "    lemma = data.drop(columns=['tweet_text'], axis=1)\n",
    "    lemma['tweet_text'] = frase_lemma\n",
    "    df_lemma.append(lemma)\n",
    "    \n",
    "    if i == 0:\n",
    "        display(Markdown(\"### Lemmatization com stopwords\"))\n",
    "        display(frase_lemma.head())\n",
    "    \n",
    "stemming = pd.concat(df_stemmer)\n",
    "stemming.to_csv(\"../data/processed/stemming_com_stopwords.csv\")\n",
    "\n",
    "lemmatization = pd.concat(df_lemma)\n",
    "lemmatization.to_csv(\"../data/processed/lemmatization_com_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2a682-3def-4114-a142-202c7dec4a6f",
   "metadata": {},
   "source": [
    "## Stemming e lemmatization sem stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeafb7ca-25bb-4860-8de2-740fc0a20aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Conjunto de dados tweet_text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    @Laranjito76 A pessoa certa para isso seria o ...\n",
       "1    @behin_d_curtain Para mim, é precisamente o co...\n",
       "2    Vou fazer um video hoje... estou pensando em f...\n",
       "3    aaaaaaaa amei tanto essas polaroids, nem sei e...\n",
       "4    Valoriza o coração do menininho que vc tem. El...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Frases tokenizadas"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    [@, Laranjito76, A, pessoa, certa, para, isso,...\n",
       "1    [@, behin_d_curtain, Para, mim, ,, é, precisam...\n",
       "2    [Vou, fazer, um, video, hoje, ..., estou, pens...\n",
       "3    [aaaaaaaa, amei, tanto, essas, polaroids, ,, n...\n",
       "4    [Valoriza, o, coração, do, menininho, que, vc,...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Frases tokenizadas sem stopwords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0          [Laranjito76, pessoa, certa, vale, azevedo]\n",
       "1    [behin_d_curtain, mim, precisamente, contrário...\n",
       "2    [Vou, fazer, video, hoje, ..., pensando, falar...\n",
       "3    [aaaaaaaa, amei, tanto, polaroids, sei, expres...\n",
       "4    [Valoriza, coração, menininho, vc, diferente, ...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Stemming sem stopwords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0              [laranjito76, pesso, cert, val, azeved]\n",
       "1    [behin_d_curtain, mim, precis, contr, vem, chu...\n",
       "2    [vou, faz, vide, hoj, ..., pens, fal, pouc, so...\n",
       "3    [aaaaaaa, ame, tant, polaroid, sei, express, q...\n",
       "4    [valoriz, coraçã, menin, vc, difer, faç, sorr,...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Lemmatization sem stopwords"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    ['Laranjito76','pessoa','certo','valer','azeve...\n",
       "1    ['behin_d_curtain','eu','precisamente','contrá...\n",
       "2    ['Vou','fazer','video','hoje','...','pensar','...\n",
       "3    ['aaaaaaaar','ameir','tanto','polaroids','sabe...\n",
       "4    ['Valoriza','coração','menininho','vc','difere...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stemmer_sem_stopwords = []\n",
    "df_lemma_sem_stopwords = []\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese') + list(punctuation))\n",
    "\n",
    "for x,data in enumerate(df):\n",
    "    # Correção do fuso horário\n",
    "    data['tweet_date'] = pd.to_datetime(data['tweet_date'])\n",
    "    data['tweet_date'] = data.tweet_date.dt.tz_convert('Brazil/East')\n",
    "    \n",
    "    if x == 0:\n",
    "        display(Markdown(\"### Conjunto de dados tweet_text\"))\n",
    "        display(data['tweet_text'].head())\n",
    "    \n",
    "    # Tokenizando frases \n",
    "    word_tokens = data.apply(lambda row: word_tokenize(row[\"tweet_text\"]), axis=1)\n",
    "    if x == 0:\n",
    "        display(Markdown(\"### Frases tokenizadas\"))\n",
    "        display(word_tokens.head())\n",
    "    \n",
    "    # Removendo stopwords\n",
    "    frase_sem_stopword = word_tokens.apply(lambda x: [word for word in x if not word.lower() in stop_words])\n",
    "    \n",
    "    if x == 0:\n",
    "        display(Markdown(\"### Frases tokenizadas sem stopwords\"))\n",
    "        display(frase_sem_stopword.head())\n",
    "        \n",
    "        \n",
    "    #Stemming \n",
    "    # Criando objeto stemmer\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    \n",
    "    # Realizando stemmer nas frases\n",
    "    frase_stemmer_sem_stopword = frase_sem_stopword.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "    # Adicionando frases stemming ao dataframe\n",
    "    stemmer_sem_stopword = data.drop(columns=['tweet_text'], axis=1)\n",
    "    stemmer_sem_stopword['tweet_text'] = frase_stemmer_sem_stopword\n",
    "    df_stemmer_sem_stopwords.append(stemmer_sem_stopword)\n",
    "    \n",
    "    if x == 0:\n",
    "        display(Markdown(\"### Stemming sem stopwords\"))\n",
    "        display(frase_stemmer_sem_stopword.head())\n",
    "    \n",
    "    # Criando lista com todas as palavras\n",
    "    doc_sem_stopword = frase_sem_stopword.apply(lambda x: nlp(str([y for y in x])))\n",
    "    \n",
    "    # Realizando lematização\n",
    "    frase_lema_sem_stopwords = doc_sem_stopword.apply(lambda row: \"\".join([w.lemma_ for w in row]))\n",
    "    lemma_sem_stopwrds = data.drop(columns=['tweet_text'], axis=1)\n",
    "    lemma_sem_stopwrds['tweet_text'] = frase_lema_sem_stopwords\n",
    "    df_lemma_sem_stopwords.append(lemma_sem_stopwrds)\n",
    "    \n",
    "    if x == 0:\n",
    "        display(Markdown(\"### Lemmatization sem stopwords\"))\n",
    "        display(frase_lema_sem_stopwords.head())\n",
    "    \n",
    "stemming_sem_stopwords = pd.concat(df_stemmer_sem_stopwords)\n",
    "stemming_sem_stopwords.to_csv(\"../data/processed/stemming_sem_stopwords.csv\")\n",
    "\n",
    "lemmatization_sem_stopwords = pd.concat(df_lemma_sem_stopwords)\n",
    "lemmatization_sem_stopwords.to_csv(\"../data/processed/lemmatization_sem_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66599b52-2a60-4658-8472-bf6bc94e0e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
